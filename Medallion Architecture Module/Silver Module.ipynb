{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc297413-a390-4d17-b3a8-ef391a754929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Configuration/config file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a878d2-eb83-4786-b4a6-b50bc76fbe65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "class UpsertCls():\n",
    "\n",
    "    def __init__(self, merge_query, view_name, delete_query=None):\n",
    "        self.query = merge_query\n",
    "        self.view_name = view_name\n",
    "        self.delete_query = delete_query\n",
    "    \n",
    "    def upsert(self, df, batch_id):\n",
    "        df.createOrReplaceTempView(self.view_name)\n",
    "        df._jdf.sparkSession().sql(self.query)\n",
    "    \n",
    "    def multiUpsert(self, df, batch_id):\n",
    "        df_new_user = df.filter(col(\"update_type\") == \"new\").drop(\"update_type\")\n",
    "        df_update_user = df.filter(col(\"update_type\") == \"update\").drop(\"update_type\")\n",
    "        df_delete_user = df.filter(col(\"update_type\")==\"delete\").drop(\"update_type\")\n",
    "        if df_new_user.count()>0:\n",
    "            self.upsert(df_new_user, batch_id)\n",
    "        if df_delete_user.count()>0:\n",
    "            ls_user_id = df_delete_user.select(\"user_id\").distinct().collect()\n",
    "            delete_query = self.delete_query.format(ls_user_id)\n",
    "            df_delete_user._jdf.sparkSession().sql(delete_query)\n",
    "        window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"timestamp\").desc())\n",
    "        df_update_user = df_update_user.withColumn('rank', rank().over(window_spec))\n",
    "        final_df = df_update_user.where(col(\"rank\")==1).drop(\"rank\")\n",
    "        self.upsert(final_df, batch_id)        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b981d374-a18b-4982-ae6a-e3e8a092002f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import cast, col, to_date, from_unixtime, to_timestamp, from_json, round, when, min, max, broadcast, date_diff, current_date, floor\n",
    "\n",
    "\n",
    "class Silver():\n",
    "\n",
    "    def __init__(self):\n",
    "        obj_conf = ConfigModule()\n",
    "        self.checkpoint_dir = f\"{obj_conf.base_checkpoint_path}/checkpoint\"\n",
    "        self.catalog = obj_conf.environment\n",
    "        self.schema = obj_conf.db_name\n",
    "\n",
    "    \n",
    "    def get_sl_users(self, version=0, once=True, processingtime='15 seconds'):\n",
    "        print(f'Streaming for {self.catalog}.{self.schema}.sl_users table...', end='')\n",
    "        read_df = spark.readStream.option('startingVersion', version)\\\n",
    "                            .option('ignoreDeletes', True)\\\n",
    "                            .table(f\"{self.catalog}.{self.schema}.bz_registered_users\")\n",
    "        updated_df = read_df.select(\"user_id\", \"device_id\", \"mac_address\", \n",
    "                                    to_timestamp(col(\"registration_timestamp\").cast('long')).alias(\"registration_timestamp\"))\\\n",
    "                            .dropDuplicates([\"user_id\", \"device_id\"])\n",
    "        merge_query = f\"\"\" MERGE INTO {self.catalog}.{self.schema}.sl_users AS target USING users_view AS source \n",
    "                            ON source.user_id=target.user_id AND source.device_id=target.device_id \n",
    "                            WHEN NOT MATCHED THEN INSERT *;\"\"\"\n",
    "        obj_upsertcls = UpsertCls(merge_query, 'users_view')\n",
    "        final_df = updated_df.writeStream.queryName('sl_users_stream')\\\n",
    "                                    .format('delta')\\\n",
    "                                    .option('checkpointLocation', f'{self.checkpoint_dir}/cp_sl_users')\\\n",
    "                                    .outputMode('update')\\\n",
    "                                    .foreachBatch(obj_upsertcls.upsert)\n",
    "        if once:\n",
    "            final_df.trigger(availableNow = once).start()\n",
    "        else:\n",
    "            final_df.trigger(processingTime = processingtime).start()\n",
    "        print('Started.')\n",
    "    \n",
    "    def get_sl_gym_logins(self, version=0, once=True, processingtime='15 seconds'):\n",
    "        print(f'Streaming for {self.catalog}.{self.schema}.sl_gym_logins table...', end='')\n",
    "        read_df = spark.readStream.option('startingVersion', version)\\\n",
    "                            .option('ignoreDeletes', True)\\\n",
    "                            .table(f\"{self.catalog}.{self.schema}.bz_gym_logins\")\n",
    "        updated_df = read_df.select(\"mac_address\", \"gym\",\n",
    "                                    to_timestamp(col(\"login\").cast('long')).alias(\"login\"), \n",
    "                                    to_timestamp(col(\"logout\").cast('long')).alias(\"logout\"))\\\n",
    "                            .dropDuplicates([\"mac_address\", \"gym\", \"login\", \"logout\"])\n",
    "        merge_query = f\"\"\" MERGE INTO {self.catalog}.{self.schema}.sl_gym_logins AS target \n",
    "                            USING gym_logins_view AS source \n",
    "                            ON source.mac_address=target.mac_address AND source.gym=target.gym AND source.login=target.login\n",
    "                            WHEN MATCHED AND source.logout > target.login AND source.logout > target.logout\n",
    "                            THEN UPDATE SET target.logout=source.logout\n",
    "                            WHEN NOT MATCHED THEN INSERT *;\"\"\"\n",
    "        obj_upsertcls = UpsertCls(merge_query, 'gym_logins_view')\n",
    "        final_df = updated_df.writeStream.queryName('sl_gym_logins_stream')\\\n",
    "                                    .format('delta')\\\n",
    "                                    .option('checkpointLocation', f'{self.checkpoint_dir}/cp_sl_gym_logins')\\\n",
    "                                    .outputMode('update')\\\n",
    "                                    .foreachBatch(obj_upsertcls.upsert)\n",
    "        if once:\n",
    "            final_df.trigger(availableNow = once).start()\n",
    "        else:\n",
    "            final_df.trigger(processingTime = processingtime).start()\n",
    "        print('Started.')\n",
    "    \n",
    "    def get_sl_user_profile(self, version=0, once=True, processingtime='15 seconds'):\n",
    "        print(f'Streaming for {self.catalog}.{self.schema}.sl_user_profile table...', end='')\n",
    "        user_profile_schema = \"user_id bigint, update_type string, timestamp double, dob string, sex string, gender string, first_name string, last_name string, address struct<street_address:string, city:string, state:string, zip:bigint>\"\n",
    "        merge_query = f\"\"\"MERGE INTO {self.catalog}.{self.schema}.sl_user_profile AS target\n",
    "                        USING user_profile_view AS source ON source.user_id=target.user_id\n",
    "                        WHEN MATCHED THEN UPDATE SET  \n",
    "                        target.timestamp=source.timestamp, \n",
    "                        target.dob=source.dob, \n",
    "                        target.sex=source.sex, \n",
    "                        target.gender=source.gender, \n",
    "                        target.first_name=source.first_name, \n",
    "                        target.last_name=source.last_name, \n",
    "                        target.street_address=source.street_address, \n",
    "                        target.city=source.city, \n",
    "                        target.state=source.state, \n",
    "                        target.zip=source.zip\n",
    "                        WHEN NOT MATCHED THEN INSERT *;\"\"\"\n",
    "        delete_query = f\"\"\"DELETE FROM {self.catalog}.{self.schema}.sl_user_profile where user_id={0}\"\"\"\n",
    "        read_df = spark.readStream.option('StartingVersion', version)\\\n",
    "                                    .option('ignoreDeletes', True)\\\n",
    "                                    .table(f'{self.catalog}.{self.schema}.bz_kafka_multiplex')\\\n",
    "                                    .where(\"topic='user_info'\")\n",
    "        schema_df = read_df.select(from_json(col(\"value\"), user_profile_schema).alias('a')).select(\"a.*\")\\\n",
    "                            .select(\"user_id\", \"update_type\", to_timestamp(col(\"timestamp\")).alias(\"timestamp\"), to_date(col(\"dob\"), \"MM/dd/yyyy\").alias(\"dob\"), \"sex\", \"gender\", \"first_name\", \"last_name\",\"address.street_address\", \"address.city\",\"address.state\", \"address.zip\")\n",
    "        updated_df = schema_df.dropDuplicates([\"user_id\", \"update_type\", \"timestamp\"])\n",
    "        obj_upsertcls = UpsertCls(merge_query, 'user_profile_view', delete_query=delete_query)\n",
    "        final_df = updated_df.writeStream.queryName('sl_user_profile_stream')\\\n",
    "                                        .format('delta')\\\n",
    "                                        .option(\"checkpointLocation\", f'{self.checkpoint_dir}/cp_sl_user_profile')\\\n",
    "                                        .outputMode('update')\\\n",
    "                                        .foreachBatch(obj_upsertcls.multiUpsert)\n",
    "        if once:\n",
    "            final_df.trigger(availableNow = once).start()\n",
    "        else:\n",
    "            final_df.trigger(processingTime = processingtime).start()\n",
    "        print('Started.')\n",
    "    \n",
    "    def get_sl_heart_rate(self, version=0, once=True, processingtime='15 seconds'):\n",
    "        print(f'Streaming for {self.catalog}.{self.schema}.sl_heart_rate table...', end='')\n",
    "        read_df = spark.readStream.option('startingVersion', version)\\\n",
    "                            .option('ignoreDeletes', True)\\\n",
    "                            .table(f\"{self.catalog}.{self.schema}.bz_kafka_multiplex\")\\\n",
    "                            .where(\"topic = 'bpm'\")\n",
    "        heart_rate_schema = \"device_id long, time double, heartrate double\"\n",
    "        schema_df = read_df.select(from_json(col(\"value\"), heart_rate_schema).alias('a')).select(\"a.*\")\\\n",
    "                            .select(\"device_id\", to_timestamp(col(\"time\")).alias(\"time\"), round(col(\"heartrate\"),4).alias(\"heart_rate\"))\n",
    "        updated_df = schema_df.dropDuplicates([\"device_id\", \"time\"])\n",
    "        merge_query = f\"\"\" MERGE INTO {self.catalog}.{self.schema}.sl_heart_rate AS target \n",
    "                            USING heart_rate_view AS source \n",
    "                            ON source.device_id=target.device_id AND source.time=target.time\n",
    "                            WHEN NOT MATCHED THEN INSERT *;\"\"\"\n",
    "        obj_upsertcls = UpsertCls(merge_query, 'heart_rate_view')\n",
    "        final_df = updated_df.writeStream.queryName('sl_heart_rate_stream')\\\n",
    "                                    .format('delta')\\\n",
    "                                    .option('checkpointLocation', f'{self.checkpoint_dir}/cp_sl_heart_rate')\\\n",
    "                                    .outputMode('update')\\\n",
    "                                    .foreachBatch(obj_upsertcls.upsert)\n",
    "        if once:\n",
    "            final_df.trigger(availableNow = once).start()\n",
    "        else:\n",
    "            final_df.trigger(processingTime = processingtime).start()\n",
    "        print('Started.')\n",
    "    \n",
    "\n",
    "    def get_sl_workout_session(self, version=0, once=True, processingtime='15 seconds'):\n",
    "        print(f'Streaming for {self.catalog}.{self.schema}.sl_workout_session table...', end='')\n",
    "        read_df = spark.readStream.option('startingVersion', version)\\\n",
    "                            .option('ignoreDeletes', True)\\\n",
    "                            .table(f\"{self.catalog}.{self.schema}.bz_kafka_multiplex\")\\\n",
    "                            .where(\"topic = 'workout'\")\n",
    "        workout_schema = \"\"\"user_id long, workout_id long, timestamp double, \n",
    "                            action string, session_id long\"\"\"\n",
    "        schema_df = read_df.select(from_json(col(\"value\"), workout_schema).alias('a')).select(\"a.*\")\\\n",
    "                            .select(\"user_id\", \"workout_id\", to_timestamp(col(\"timestamp\")).alias(\"timestamp\"), \"action\", \"session_id\")\n",
    "        updated_df = schema_df.dropDuplicates([\"user_id\", \"timestamp\"])\n",
    "        merge_query = f\"\"\" MERGE INTO {self.catalog}.{self.schema}.sl_workout_session AS target \n",
    "                            USING workout_session_view AS source \n",
    "                            ON source.user_id=target.user_id AND source.timestamp=target.timestamp\n",
    "                            WHEN NOT MATCHED THEN INSERT (user_id, workout_id, timestamp, action, session_id)\n",
    "                            VALUES (source.user_id, source.workout_id, source.timestamp, source.action, source.session_id);\"\"\"\n",
    "        obj_upsertcls = UpsertCls(merge_query, 'workout_session_view')\n",
    "        final_df = updated_df.writeStream.queryName('sl_workout_session_stream')\\\n",
    "                                .format('delta')\\\n",
    "                                .option('checkpointLocation', f'{self.checkpoint_dir}/cp_sl_workout_session')\\\n",
    "                                .outputMode('update')\\\n",
    "                                .foreachBatch(obj_upsertcls.upsert)\n",
    "        if once:\n",
    "            final_df.trigger(availableNow = once).start()\n",
    "        else:\n",
    "            final_df.trigger(processingTime = processingtime).start()\n",
    "        print('Started.')\n",
    "    \n",
    "    def get_sl_complete_workout(self, version=0, once=True, processingtime='15 seconds'):\n",
    "        print(f\"Streaming for {self.catalog}.{self.schema}.sl_complete_workout table...\", end='')\n",
    "        cw_merge_query = f\"\"\"MERGE INTO {self.catalog}.{self.schema}.sl_complete_workout AS target\n",
    "                                    USING complete_workout_view AS source ON \n",
    "                                    source.user_id=target.user_id AND source.session_id=target.session_id AND \n",
    "                                    source.workout_id=target.workout_id\n",
    "                                    WHEN MATCHED AND source.end_time > target.start_time AND source.end_time > target.end_time THEN UPDATE SET target.end_time=source.end_time\n",
    "                                    WHEN NOT MATCHED THEN INSERT *\"\"\"\n",
    "        read_df = spark.readStream.option('startingVersion', version)\\\n",
    "                                    .option('ignoreDeleted', True)\\\n",
    "                                    .table(f'{self.catalog}.{self.schema}.sl_workout_session')\n",
    "        updated_df = read_df.withColumn(\"start_time\", when(col(\"action\")==\"start\", col(\"timestamp\")))\\\n",
    "                            .withColumn(\"end_time\", when(col(\"action\")==\"stop\", col(\"timestamp\")))\\\n",
    "                            .groupBy(\"user_id\", \"workout_id\", \"session_id\")\\\n",
    "                            .agg(min(\"start_time\").alias(\"start_time\"), max(\"end_time\").alias(\"end_time\"))\\\n",
    "                            .select(\"user_id\", \"workout_id\", \"session_id\", \"start_time\", \"end_time\")\n",
    "        obj_upsertcls = UpsertCls(cw_merge_query, 'complete_workout_view')\n",
    "        final_df = updated_df.writeStream.queryName('sl_complete_workout_stream')\\\n",
    "                            .option('checkpointLocation', f'{self.checkpoint_dir}/cp_sl_complete_workout')\\\n",
    "                            .outputMode('update')\\\n",
    "                            .foreachBatch(obj_upsertcls.upsert)\n",
    "        if once:\n",
    "            time.sleep(60)\n",
    "            final_df.trigger(availableNow=once).start()\n",
    "        else:\n",
    "            final_df.trigger(processingTime=processingtime).start()\n",
    "        print('Started.')\n",
    "\n",
    "    def get_sl_user_bins(self, version=0, once=True, processingtime='15 seconds'):\n",
    "        print(f\"Streaming for {self.catalog}.{self.schema}.sl_user_bins table...\", end='')\n",
    "        merge_query = f\"\"\"MERGE INTO {self.catalog}.{self.schema}.sl_user_bins AS target\n",
    "                            USING user_bins_view AS source ON \n",
    "                            source.user_id=target.user_id\n",
    "                            WHEN MATCHED THEN UPDATE SET *\n",
    "                            WHEN NOT MATCHED THEN INSERT *;\n",
    "                        \"\"\"\n",
    "        register_user_df = spark.readStream.option('startingVersion', version)\\\n",
    "                                    .option('ignoreDeleted', True)\\\n",
    "                                    .table(f'{self.catalog}.{self.schema}.sl_users')\n",
    "        profile_df = spark.read.option('startingVersion', version)\\\n",
    "                            .option('ignoreDeleted', True)\\\n",
    "                            .table(f\"{self.catalog}.{self.schema}.sl_user_profile\")\\\n",
    "                            .withColumn( \"age_count\", floor( date_diff(current_date(), to_date(\"dob\")) / 365 ))\\\n",
    "                            .withColumn(\"age\",when(col(\"age_count\")<18, \"under 18\")\\\n",
    "                                            .when((col(\"age_count\")>17) & (col(\"age_count\")<=25), \"18-25\")\\\n",
    "                                            .when((col(\"age_count\")>25) & (col(\"age_count\")<=30), \"26-30\")\\\n",
    "                                            .when((col(\"age_count\")>30) & (col(\"age_count\")<=35), \"31-35\")\\\n",
    "                                            .when((col(\"age_count\")>35) & (col(\"age_count\")<=40), \"36-40\")\\\n",
    "                                            .when((col(\"age_count\")>40) & (col(\"age_count\")<=45), \"41-45\")\\\n",
    "                                            .when((col(\"age_count\")>45) & (col(\"age_count\")<=50), \"46-50\")\\\n",
    "                                            .when((col(\"age_count\")>50) & (col(\"age_count\")<=55), \"51-55\")\\\n",
    "                                            .when((col(\"age_count\")>55) & (col(\"age_count\")<=60), \"56-60\")\\\n",
    "                                            .when((col(\"age_count\")>60) & (col(\"age_count\")<=60), \"56-60\")\\\n",
    "                                            .when((col(\"age_count\")>75) & (col(\"age_count\")<=100), \"above 75\")\\\n",
    "                                            .otherwise(\"invalid age\"))\n",
    "        combine_df = register_user_df.join(profile_df, on=\"user_id\", how=\"left\")\\\n",
    "                        .fillna({\"age\":\"Unknown\", \"gender\":\"Unknown\", \"city\":\"Unknown\", \"state\":\"Unknown\"})\\\n",
    "                        .select(\"user_id\", \"device_id\", \"age\", \"gender\", \"city\", \"state\")\n",
    "        obj_upsertcls = UpsertCls(merge_query, 'user_bins_view')\n",
    "        final_df = combine_df.writeStream.queryName('sl_user_bin_stream')\\\n",
    "                            .option('checkpointLocation', f'{self.checkpoint_dir}/cp_sl_user_bin')\\\n",
    "                            .outputMode('update')\\\n",
    "                            .foreachBatch(obj_upsertcls.upsert)\n",
    "        if once:\n",
    "            time.sleep(60)\n",
    "            final_df.trigger(availableNow=once).start()\n",
    "        else:\n",
    "            final_df.trigger(processingTime=processingtime).start()\n",
    "        print('Started.')\n",
    "\n",
    "        \n",
    "\n",
    "    def launcher(self, once=True, processingtime='15 seconds'):\n",
    "        self.get_sl_users(once=once, processingtime=processingtime)\n",
    "        self.get_sl_gym_logins(once=once, processingtime=processingtime)\n",
    "        self.get_sl_user_profile(once=once, processingtime=processingtime)\n",
    "        self.get_sl_heart_rate(once=once, processingtime=processingtime)\n",
    "        self.get_sl_workout_session(once=once, processingtime=processingtime)\n",
    "        self.get_sl_complete_workout(once=once, processingtime=processingtime)\n",
    "        self.get_sl_user_bins(once=once, processingtime=processingtime)\n",
    "        for stream in spark.streams.active:\n",
    "            stream.awaitTermination()\n",
    "\n",
    "\n",
    "\n",
    "#obj = Silver()\n",
    "#obj.launcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ecf9cb6-9f07-48f8-8c5c-790c8a41bdb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SilverTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        obj_conf = ConfigModule()\n",
    "        self.catalog = obj_conf.environment\n",
    "        self.schema = obj_conf.db_name\n",
    "\n",
    "    def assert_fn(self, table_name, filter, expected_count):\n",
    "        print(f'Testing Silver layer - {self.catalog}.{self.schema}.{table_name} table...', end='')\n",
    "        actual_count = spark.sql(f\"select count(*) from {self.catalog}.{self.schema}.{table_name} where {filter}\").collect()[0][0]\n",
    "        assert actual_count==expected_count, f\"Test case failed, actual count is {actual_count}\"\n",
    "        print('Test Passed.')\n",
    "    \n",
    "    def testcases(self):\n",
    "        self.assert_fn('sl_users', 'true', 5)\n",
    "        self.assert_fn('sl_gym_logins', 'gym==1', 2)\n",
    "        self.assert_fn('sl_user_profile', \"true\", 2)\n",
    "        self.assert_fn('sl_heart_rate', \"true\", 5)\n",
    "        self.assert_fn('sl_workout_session', \"true\", 2)\n",
    "        self.assert_fn('sl_complete_workout', \"user_id=12474\", 1)\n",
    "        self.assert_fn('sl_user_bins', \"user_id in (12140, 12474)\", 2)\n",
    "\n",
    "#obj = BronzeTestSuite()\n",
    "#obj.testcases()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver Module",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}