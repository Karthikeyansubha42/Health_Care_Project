{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c2cb7c0-cdab-4c61-8027-9910c33e6474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Configuration/config file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922a40e0-6e7d-4cb4-9a7c-75e220812246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, LongType, IntegerType, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "class Bronze():\n",
    "\n",
    "    def __init__(self):\n",
    "        obj_conf = ConfigModule()\n",
    "        self.checkpoint_dir = f\"{obj_conf.base_checkpoint_path}/checkpoint\"\n",
    "        self.raw_file_dir = f\"{obj_conf.base_file_path}/raw\"\n",
    "        self.catalog = obj_conf.environment\n",
    "        self.schema = obj_conf.db_name\n",
    "        self.bootstrap_server = obj_conf.kafka_bootstrap_servers\n",
    "        self.username = obj_conf.kafka_username\n",
    "        self.password = obj_conf.kafka_password\n",
    "    \n",
    "    def get_registered_user(self, once=True, processingTime='10 seconds'):\n",
    "        print('Consuming registered user details...', end='')\n",
    "        self.reg_user_schema = StructType([\n",
    "                                            StructField('user_id', LongType()),\n",
    "                                            StructField('device_id', LongType()),\n",
    "                                            StructField('mac_address', StringType()),\n",
    "                                            StructField('registration_timestamp', StringType())\n",
    "                                        ])\n",
    "        df = spark.readStream.format('cloudFiles')\\\n",
    "                            .option('cloudFiles.format', 'csv')\\\n",
    "                            .option('maxFilePerTrigger', 1)\\\n",
    "                            .option('header', True)\\\n",
    "                            .schema(self.reg_user_schema)\\\n",
    "                            .load(f'{self.raw_file_dir}/reg_user')\n",
    "        new_col_df = df.withColumn('load_time', f.current_timestamp())\\\n",
    "                       .withColumn('source_file', f.input_file_name())\n",
    "        reg_user_Squery = new_col_df.writeStream.queryName('bz_registered_user_stream')\\\n",
    "                            .format('delta')\\\n",
    "                            .option('checkpointLocation', f'{self.checkpoint_dir}/cp_registered_user')\\\n",
    "                            .outputMode('append')\n",
    "        if once:\n",
    "            reg_user_Squery = reg_user_Squery.trigger(availableNow=True).toTable(f'{self.catalog}.{self.schema}.bz_registered_users')\n",
    "        else:\n",
    "            reg_user_Squery.trigger(processingTime=processingTime).toTable(f'{self.catalog}.{self.schema}.bz_registered_users')\n",
    "        print('Done.')\n",
    "        #reg_user_Squery.awaitTermination()\n",
    "\n",
    "    def get_gym_logins(self, once=True, processingTime='10 seconds'):\n",
    "        print('Consuming gym login details...', end='')\n",
    "        self.gym_login_schema = StructType([\n",
    "                                            StructField('mac_address', StringType()),\n",
    "                                            StructField('gym', LongType()),\n",
    "                                            StructField('login', StringType()),\n",
    "                                            StructField('logout', StringType())\n",
    "                                        ])\n",
    "        df = spark.readStream.format('cloudFiles')\\\n",
    "                            .option('cloudFiles.format', 'csv')\\\n",
    "                            .option('maxFilePerTrigger', 1)\\\n",
    "                            .option('header', True)\\\n",
    "                            .schema(self.gym_login_schema)\\\n",
    "                            .load(f'{self.raw_file_dir}/gym_login')\n",
    "        new_col_df = df.withColumn('load_time', f.current_timestamp())\\\n",
    "                       .withColumn('source_file', f.input_file_name())\n",
    "        gym_login_Squery = new_col_df.writeStream.queryName('bz_gym_logins_stream')\\\n",
    "                            .format('delta')\\\n",
    "                            .option('checkpointLocation', f'{self.checkpoint_dir}/cp_gym_logins')\\\n",
    "                            .outputMode('append')\n",
    "        if once:\n",
    "            gym_login_Squery.trigger(availableNow=True).toTable(f'{self.catalog}.{self.schema}.bz_gym_logins')\n",
    "        else:\n",
    "            gym_login_Squery.trigger(processingTime=processingTime).toTable(f'{self.catalog}.{self.schema}.bz_gym_logins')\n",
    "        print('Done.')\n",
    "        #gym_login_Squery.awaitTermination()\n",
    "        \n",
    "    \n",
    "    def get_kafka_multiplex(self, once=True, processingTime='10 seconds'):\n",
    "        print('Consuming kafka multiplex details...', end='')\n",
    "        topic = \"user_info, bpm, workout\"\n",
    "        kafka_df = spark.readStream.format('kafka')\\\n",
    "                            .option('kafka.bootstrap.servers', self.bootstrap_server)\\\n",
    "                            .option('kafka.security.protocol', 'SASL_SSL')\\\n",
    "                            .option('kafka.sasl.mechanism', 'PLAIN')\\\n",
    "                            .option('kafka.sasl.jaas.config', f\"\"\"org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{self.username}\" password=\"{self.password}\";\"\"\")\\\n",
    "                            .option('maxOffsetPerTrigger', 1)\\\n",
    "                            .option('subscribe', topic)\\\n",
    "                            .option('startingOffsets', 'earliest')\\\n",
    "                            .load()\n",
    "        kafka_df = kafka_df.select(f.col('key').cast('string'), f.col('value').cast('string'), 'topic', 'partition', 'offset', f.col('timestamp').cast('bigint'))\n",
    "        date_df = spark.table(f'{self.catalog}.{self.schema}.date_lookup').select('date', 'week_part')\n",
    "        updated_df = kafka_df.join(date_df, f.to_date(f.from_unixtime(kafka_df.timestamp))==date_df.date, 'left')\\\n",
    "                            .withColumn('load_time', f.current_timestamp())\\\n",
    "                            .withColumn('source_file', f.input_file_name())\n",
    "        kafka_multiplex_Squery = updated_df.writeStream.queryName('bl_kafka_multiplex_stream')\\\n",
    "                            .format('delta')\\\n",
    "                            .option('checkpointLocation', f'{self.checkpoint_dir}/cp_kafka_multiplex_logins')\\\n",
    "                            .outputMode('append')\n",
    "        if once:\n",
    "            kafka_multiplex_Squery.trigger(availableNow=True).toTable(f'{self.catalog}.{self.schema}.bz_kafka_multiplex')\n",
    "        else:\n",
    "            kafka_multiplex_Squery.trigger(processingTime=processingTime).toTable(f'{self.catalog}.{self.schema}.bz_kafka_multiplex')\n",
    "        print('Done.')\n",
    "        #kafka_multiplex_Squery.awaitTermination()\n",
    "    \n",
    "    def launcher(self, once=True, processingtime='5 seconds'):\n",
    "        self.get_registered_user(once, processingtime)\n",
    "        self.get_gym_logins(once, processingtime)\n",
    "        self.get_kafka_multiplex(once, processingtime)\n",
    "        for stream in spark.streams.active:\n",
    "            stream.awaitTermination()\n",
    "\n",
    "#obj = Bronze()\n",
    "#obj.launcher()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de32d22-cd1d-4e7f-83de-1aa060d7a66f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class BronzeTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        obj_conf = ConfigModule()\n",
    "        self.catalog = obj_conf.environment\n",
    "        self.schema = obj_conf.db_name\n",
    "\n",
    "    def assert_fn(self, table_name, filter, expected_count):\n",
    "        print(f'Testing bronze layer - {self.catalog}.{self.schema}.{table_name} table...', end='')\n",
    "        actual_count = spark.sql(f\"select count(*) from {self.catalog}.{self.schema}.{table_name} where {filter}\").collect()[0][0]\n",
    "        assert actual_count==expected_count, f\"Test case failed, actual count is {actual_count}\"\n",
    "        print('Test Passed.')\n",
    "    \n",
    "    def testcases(self):\n",
    "        self.assert_fn('bz_registered_users', 'true', 5)\n",
    "        self.assert_fn('bz_gym_logins', 'true', 8)\n",
    "        self.assert_fn('bz_kafka_multiplex', \"topic='user_info'\", 4)\n",
    "        self.assert_fn('bz_kafka_multiplex', \"topic='bpm'\", 5)\n",
    "        self.assert_fn('bz_kafka_multiplex', \"topic='workout'\", 2)\n",
    "\n",
    "#obj = BronzeTestSuite()\n",
    "#obj.testcases()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7085732637864139,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze Module",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}