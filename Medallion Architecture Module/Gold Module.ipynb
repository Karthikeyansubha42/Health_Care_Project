{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16bf301e-5d29-46d0-a53d-c2e647f6883d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Configuration/config file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d27f466-26d2-4bea-bda2-5e0f9a67a0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class UpsertCls():\n",
    "\n",
    "    def __init__(self, merge_query, view_name):\n",
    "        self.query = merge_query\n",
    "        self.view_name = view_name\n",
    "\n",
    "    def upsert(self, df, batch_id):\n",
    "        df.createOrReplaceTempView(self.view_name)\n",
    "        df._jdf.sparkSession().sql(self.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f77d5f-aac8-4b82-b70d-747a69325074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, min, avg, max, col\n",
    "\n",
    "class Gold():\n",
    "\n",
    "    def __init__(self):\n",
    "        obj_conf = ConfigModule()\n",
    "        self.checkpoint_dir = f\"{obj_conf.base_checkpoint_path}/checkpoint\"\n",
    "        self.catalog = obj_conf.environment\n",
    "        self.schema = obj_conf.db_name\n",
    "    \n",
    "    def get_gd_complete_bpm_summary(self, version=0, once=True, processingtime='15 seconds'):\n",
    "        print(f\"Steaming started for {self.catalog}.{self.schema}.gd_workout_bpm_summary table...\", end='')\n",
    "        merge_query = f\"\"\" MERGE INTO {self.catalog}.{self.schema}.gd_workout_bpm_summary AS target\n",
    "                            USING complete_bpm_summary_view AS source ON \n",
    "                            source.user_id=target.user_id AND source.date=target.date \n",
    "                            AND source.workout_id = target.workout_id AND source.session_id = target.session_id\n",
    "                            WHEN MATCHED THEN UPDATE SET *\n",
    "                            WHEN NOT MATCHED THEN INSERT *;\n",
    "                            \"\"\"\n",
    "        heartrate_df = spark.read.table(f\"{self.catalog}.{self.schema}.sl_heart_rate\")\n",
    "        user_bin_df = spark.read.table(f\"{self.catalog}.{self.schema}.sl_user_bins\")\n",
    "        combine_df = heartrate_df.join(user_bin_df, on=\"device_id\")\\\n",
    "                                .withColumn(\"date\", to_date(\"time\")).select(\"*\")\n",
    "        \n",
    "        read_df = spark.readStream.option(\"startingVersion\", version)\\\n",
    "                                    .option(\"ignoreDeletes\", True)\\\n",
    "                                    .table(f\"{self.catalog}.{self.schema}.sl_complete_workout\")\n",
    "        join_agg_df = read_df.join(combine_df, ((read_df.user_id==combine_df.user_id) & \n",
    "                                            (read_df.start_time<=combine_df.time) & \n",
    "                                            (read_df.end_time>=combine_df.time)), how=\"left\")\\\n",
    "                        .groupBy(read_df.user_id, \"session_id\", \"workout_id\", \"date\")\\\n",
    "                        .agg(min(\"heart_rate\").alias(\"min_bpm\"),\n",
    "                             avg(\"heart_rate\").alias(\"avg_bpm\"),\n",
    "                             max(\"heart_rate\").alias(\"max_bpm\"))\n",
    "        select_df = join_agg_df.join(user_bin_df, on=\"user_id\")\\\n",
    "                                .select(*join_agg_df.columns, \"age\", col(\"gender\").alias(\"sex\"), \"city\", \"state\")\n",
    "        cls_obj = UpsertCls(merge_query, 'complete_bpm_summary_view')\n",
    "        final_df = select_df.writeStream.queryName(\"gd_complete_bpm_summary_stream\")\\\n",
    "                                    .format(\"delta\")\\\n",
    "                                    .option(\"checkpointLocation\", f\"{self.checkpoint_dir}/cp_gd_complete_bpm_summary\")\\\n",
    "                                    .outputMode('update')\\\n",
    "                                    .foreachBatch(cls_obj.upsert)\n",
    "        if once:\n",
    "            final_df.trigger(availableNow=once).start()\n",
    "        else:\n",
    "            final_df.trigger(processingTime=processingtime).start()\n",
    "        print(\"Started.\")\n",
    "    \n",
    "    def launcher(self, once=True, processingtime='15 seconds'):\n",
    "        self.get_gd_complete_bpm_summary(once=once, processingtime=processingtime)\n",
    "\n",
    "#obj = Gold()\n",
    "#obj.launcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc09c9f-ae86-4dfc-8988-6269cf78097f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GoldTestSuite():\n",
    "\n",
    "    def __init__(self):\n",
    "        obj_conf = ConfigModule()\n",
    "        self.catalog = obj_conf.environment\n",
    "        self.schema = obj_conf.db_name\n",
    "\n",
    "    def assert_fn(self, table_name, filter, expected_count):\n",
    "        print(f'Testing Gold layer - {self.catalog}.{self.schema}.{table_name} table...', end='')\n",
    "        actual_count = spark.sql(f\"select count(*) from {self.catalog}.{self.schema}.{table_name} where {filter}\").collect()[0][0]\n",
    "        assert actual_count==expected_count, f\"Test case failed, actual count is {actual_count}\"\n",
    "        print('Test Passed.')\n",
    "    \n",
    "    def assert_data_fn(self, table_name, test_data_path):\n",
    "        print(f\"Data compare for {self.catalog}.{self.schema}.{table_name} table...\", end='')\n",
    "        test_df = spark.read.format('parquet').load(test_data_path)\n",
    "        table_df = spark.read.format(\"delta\").table(f\"{self.catalog}.{self.schema}.{table_name}\")\n",
    "        assert test_df.exceptAll(table_df).isEmpty() and table_df.exceptAll(test_df).isEmpty(), \"Data mismatch, testcase failed.\"\n",
    "        print(\"Passed.\")\n",
    "    \n",
    "    def testcases(self):\n",
    "        self.assert_fn('gd_workout_bpm_summary', 'true', 1)\n",
    "        self.assert_fn('gd_gym_summary', 'true', 8)\n",
    "        self.assert_data_fn('gd_workout_bpm_summary', '/FileStore/tables/test_workout_bpm_summary')\n",
    "        self.assert_data_fn('gd_gym_summary', '/FileStore/tables/test_gd_gym_summary')\n",
    "\n",
    "#obj = GoldTestSuite()\n",
    "#obj.testcases()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold Module",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}