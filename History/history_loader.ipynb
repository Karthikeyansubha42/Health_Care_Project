{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b14271-78ea-4f4a-a983-8aabb35b6a5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Repos/HealthCare project/HealthCare Project/Configuration/config file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fbbe938-8d4a-4912-935e-a4f7184a8958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class History():\n",
    "\n",
    "    def __init__(self):\n",
    "        obj_conf = ConfigModule()\n",
    "        self.raw_file_dir = f\"{obj_conf.base_file_path}/raw\"\n",
    "        self.catalog = obj_conf.environment\n",
    "        self.schema = obj_conf.db_name\n",
    "    \n",
    "    def history_load(self):\n",
    "        self.json_schema = StructType([StructField ('date', DateType()),\\\n",
    "                                  StructField ('week', StringType()),\n",
    "                                  StructField ('year', StringType()),\n",
    "                                  StructField ('month', StringType()),\n",
    "                                  StructField ('dayofweek', StringType()),\n",
    "                                  StructField ('dayofmonth', StringType()),\n",
    "                                  StructField ('dayofyear', StringType()),\n",
    "                                  StructField ('week_part', StringType())\n",
    "                                  ])\n",
    "        df = spark.read.format('json').schema(self.json_schema).load(self.raw_file_dir)\\\n",
    "            .withColumn('week', col('week').cast('int'))\\\n",
    "            .withColumn('year', col('year').cast('int'))\\\n",
    "            .withColumn('month', col('month').cast('int'))\\\n",
    "            .withColumn('day_of_week', col('dayofweek').cast('int'))\\\n",
    "            .withColumn('day_of_month', col('dayofmonth').cast('int'))\\\n",
    "            .withColumn('day_of_year', col('dayofyear').cast('int'))\\\n",
    "            .select('date','week','year','month','day_of_week','day_of_month','day_of_year','week_part')\n",
    "        df.write.mode(\"overwrite\").saveAsTable(f\"{self.catalog}.{self.schema}.date_lookup\")\n",
    "    \n",
    "    def launch_history(self):\n",
    "        print('Inserting history records...', end='')\n",
    "        self.history_load()\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96396e7e-8672-4fd0-9444-33a20d5fcc3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class HistoryTestSuit():\n",
    "\n",
    "    def __init__(self):\n",
    "        obj_conf = ConfigModule()\n",
    "        self.raw_file_dir = f\"{obj_conf.base_file_path}/raw\"\n",
    "        self.catalog = obj_conf.environment\n",
    "        self.schema = obj_conf.db_name\n",
    "    \n",
    "    def testcases(self, expected_count):\n",
    "        print(\"Testing History tables...\", end='')\n",
    "        actual_count = spark.sql(f\"select count(*) from {self.catalog}.{self.schema}.date_lookup\").collect()[0][0]\n",
    "        assert actual_count==expected_count, f\"Test Failed, actual count is {actual_count}\"\n",
    "        print(\"Test Passed\")\n",
    "\n",
    "#obj = HistoryTestSuit()\n",
    "#obj.testcases(expected_count=365)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "history_loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}