{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433267cd-a98a-48f1-be9b-a966c3e9267b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./config file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d19e4d3-b87b-4e4e-a876-b470df0621a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class SetupHelper():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conf = ConfigModule('dev')\n",
    "        self.data_dir = f\"{self.conf.base_file_path}/data\"\n",
    "        self.checkpoint_dir = f\"{self.conf.base_checkpoint_path}/checkpoint\"\n",
    "        self.catalog = self.conf.environment\n",
    "        self.schema = self.conf.db_name\n",
    "        self.database_intialize = False\n",
    "    \n",
    "    def create_database(self):\n",
    "        spark.catalog.clearCache()\n",
    "        print(f\"\\tCreating database {self.catalog}.{self.schema} ...\", end='')\n",
    "        spark.sql(f\"create database if not exists {self.catalog}.{self.schema}\")\n",
    "        spark.sql(f\"use {self.catalog}.{self.schema}\")\n",
    "        self.database_intialize = True\n",
    "        print('Done.')\n",
    "    \n",
    "    def create_registered_user_bz(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.bz_registered_users...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.bz_registered_users (\n",
    "                        user_id bigint, \n",
    "                        device_id bigint, \n",
    "                        mac_address string, \n",
    "                        registration_timestamp string,\n",
    "                        load_time timestamp,\n",
    "                        source_file string\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "\n",
    "    def create_gym_logins_bz(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.bz_gym_logins...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.bz_gym_logins (\n",
    "                        mac_address string, \n",
    "                        gym bigint, \n",
    "                        login string, \n",
    "                        logout string,\n",
    "                        load_time timestamp,\n",
    "                        source_file string\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "\n",
    "    def create_kafka_multiplex_bz(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.bz_kafka_multiplex...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.bz_kafka_multiplex (\n",
    "                       key string, \n",
    "                       value string, \n",
    "                       topic string, \n",
    "                       partition int, \n",
    "                       offset long, \n",
    "                       timestamp long, \n",
    "                       date date, \n",
    "                       week_part string, \n",
    "                       load_time timestamp, \n",
    "                       source_file string\n",
    "                    ) PARTITIONED BY (topic, week_part)\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_users_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_users...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_users (\n",
    "                        user_id bigint, \n",
    "                        device_id bigint, \n",
    "                        mac_address string, \n",
    "                        registration_timestamp timestamp\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_gym_logins_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_gym_logins...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_gym_logins (\n",
    "                        mac_address string, \n",
    "                        gym bigint, \n",
    "                        login timestamp, \n",
    "                        logout timestamp\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_user_profile_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_user_profile...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_user_profile (\n",
    "                        user_id bigint,\n",
    "                        update_type string,\n",
    "                        timestamp timestamp,\n",
    "                        dob string,\n",
    "                        sex string,\n",
    "                        gender string,\n",
    "                        first_name string,\n",
    "                        last_name string,\n",
    "                        address struct <\n",
    "                                        street_address string,\n",
    "                                        city string,\n",
    "                                        state string,\n",
    "                                        postalcode bigint\n",
    "                                        >\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_heart_rate_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_heart_rate...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_heart_rate (\n",
    "                        device_id bigint,\n",
    "                        time timestamp,\n",
    "                        heart_rate double\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_workout_session_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_workout_session...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_workout_session (\n",
    "                        user_id bigint,\n",
    "                        workout_id bigint,\n",
    "                        timestmap timestamp,\n",
    "                        action string,\n",
    "                        session_id bigint\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    \n",
    "    def create_workout_session_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_workout_session...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_workout_session (\n",
    "                        user_id bigint,\n",
    "                        workout_id bigint,\n",
    "                        timestmap timestamp,\n",
    "                        action string,\n",
    "                        session_id bigint\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_user_bins_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_user_bins...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_user_bins (\n",
    "                        user_id bigint,\n",
    "                        age int,\n",
    "                        gender string,\n",
    "                        city string,\n",
    "                        state string\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_complete_workout_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_complete_workout...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_complete_workout (\n",
    "                        user_id bigint,\n",
    "                        workout_id bigint,\n",
    "                        session_id bigint,\n",
    "                        start_time timestamp,\n",
    "                        end_time timestamp\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "\n",
    "    def create_date_lookup(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.date_lookup...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.date_lookup (\n",
    "                        date date,\n",
    "                        week int,\n",
    "                        year int,\n",
    "                        month int,\n",
    "                        day_of_week int,\n",
    "                        day_of_month int,\n",
    "                        day_of_year int,\n",
    "                        week_part string\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_workout_bpm_summary_gd(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.gd_workout_bpm_summary...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.gd_workout_bpm_summary (\n",
    "                        user_id bigint,\n",
    "                        date date,\n",
    "                        workout_id bigint,\n",
    "                        session_id bigint,\n",
    "                        age int,\n",
    "                        sec string,\n",
    "                        city string,\n",
    "                        state string,\n",
    "                        rec int,\n",
    "                        min_bpm double,\n",
    "                        avg_bpm double,\n",
    "                        max_bpm double\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_gym_summary_gd(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.gd_gym_summary...\", end='')\n",
    "            pass\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def launcher(self):\n",
    "        print('Started setup process: ')\n",
    "        start_time = time.time()\n",
    "        self.create_database()\n",
    "        self.create_registered_user_bz()\n",
    "        self.create_gym_logins_bz()\n",
    "        self.create_kafka_multiplex_bz()\n",
    "        self.create_users_sl()\n",
    "        self.create_workout_session_sl()\n",
    "        self.create_user_bins_sl()\n",
    "        self.create_gym_logins_sl()\n",
    "        self.create_heart_rate_sl()\n",
    "        self.create_date_lookup()\n",
    "        self.create_user_profile_sl()\n",
    "        self.create_complete_workout_sl()\n",
    "        self.create_workout_bpm_summary_gd()\n",
    "        self.create_gym_summary_gd()\n",
    "        print(f'Done in  {int(time.time() - start_time)} seconds.')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d36f201-4421-4734-b799-bc85c4a6c6f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Do you want to cleanup the database and data/checkpoint locations? (y/n):  y"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning process:\n\tDropping database dev.hc_db ...Done.\n\tDeleting & recreate checkpoint_dir abfss://hc-unmanaged-dev@healthcarelakestorage.dfs.core.windows.net/checkpoint_zone/checkpoint/cp_gym_logins ...Done.\nStarted setup process: \n\tCreating database dev.hc_db ...Done.\n\tCreating table dev.hc_db.bz_registered_users...Done.\n\tCreating table dev.hc_db.bz_gym_logins...Done.\n\tCreating table dev.hc_db.bz_kafka_multiplex...Done.\n\tCreating table dev.hc_db.sl_users...Done.\n\tCreating table dev.hc_db.sl_workout_session...Done.\n\tCreating table dev.hc_db.sl_user_bins...Done.\n\tCreating table dev.hc_db.sl_gym_logins...Done.\n\tCreating table dev.hc_db.sl_heart_rate...Done.\n\tCreating table dev.hc_db.date_lookup...Done.\n\tCreating table dev.hc_db.sl_user_profile...Done.\n\tCreating table dev.hc_db.sl_complete_workout...Done.\n\tCreating table dev.hc_db.gd_workout_bpm_summary...Done.\n\tCreating table dev.hc_db.gd_gym_summary...Done.\nDone in  8 seconds.\nValidation started...\n\tChecking if database hc_db exits or not...Exist\n\tChecking if table bz_registered_users exists in the hc_db database...Exist\n\tChecking if table bz_gym_logins exists in the hc_db database...Exist\n\tChecking if table bz_kafka_multiplex exists in the hc_db database...Exist\n\tChecking if table sl_user_profile exists in the hc_db database...Exist\n\tChecking if table sl_workout_session exists in the hc_db database...Exist\n\tChecking if table sl_user_bins exists in the hc_db database...Exist\n\tChecking if table sl_gym_logins exists in the hc_db database...Exist\n\tChecking if table sl_heart_rate exists in the hc_db database...Exist\n\tChecking if table date_lookup exists in the hc_db database...Exist\n\tChecking if table sl_complete_workout exists in the hc_db database...Exist\n\tChecking if table sl_users exists in the hc_db database...Exist\n\tChecking if table gd_workout_bpm_summary exists in the hc_db database...Exist\nDone.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "class HelperTestSuit():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cls_obj = SetupHelper()\n",
    "        self.data_dir = self.cls_obj.data_dir\n",
    "        self.checkpoint_dir = self.cls_obj.checkpoint_dir\n",
    "        self.catalog = self.cls_obj.catalog\n",
    "        self.schema = self.cls_obj.schema\n",
    "\n",
    "    def cleanup(self):\n",
    "        a = input('Do you want to cleanup the database and data/checkpoint locations? (y/n): ')\n",
    "        if a.lower()=='y':\n",
    "            print('Cleaning process:', )\n",
    "            print(f'\\tDropping database {self.catalog}.{self.schema} ...', end='')\n",
    "            spark.sql(f\"DROP DATABASE IF EXISTS {self.catalog}.{self.schema} CASCADE\")\n",
    "            print('Done.')\n",
    "            '''print(f'\\tDeleting & recreate data_dir {self.data_dir} ...', end='')\n",
    "            dbutils.fs.rm(self.data_dir, True)\n",
    "            dbutils.fs.mkdirs(self.data_dir)\n",
    "            print('Done.')'''\n",
    "            print(f'\\tDeleting & recreate checkpoint_dir {self.checkpoint_dir} ...', end='')\n",
    "            dbutils.fs.rm(self.checkpoint_dir, True)\n",
    "            dbutils.fs.mkdirs(self.checkpoint_dir)\n",
    "            print('Done.')\n",
    "        else:\n",
    "            print('Skipping cleaning process!!!')\n",
    "    \n",
    "    def assert_database_exists(self, db_name):\n",
    "        print(f'\\tChecking if database {self.schema} exits or not...', end='')\n",
    "        assert spark.sql(f\"show databases in {self.catalog}\")\\\n",
    "                           .filter(f\"databaseName='{db_name}'\").count()==1, \"Not Exist\"\n",
    "        print('Exist')\n",
    "\n",
    "    def assert_table_exists(self, table_name):\n",
    "        print(f'\\tChecking if table {table_name} exists in the {self.schema} database...', end='')\n",
    "        assert spark.sql(f\"show tables in {self.catalog}.{self.schema}\")\\\n",
    "                           .filter(f\"tableName='{table_name}' and isTemporary=False\").count()==1, \"Not Exist\"\n",
    "        print('Exist')\n",
    "    \n",
    "    def run_testcases(self):\n",
    "        self.cleanup()\n",
    "        self.cls_obj.launcher()\n",
    "        print(\"Validation started...\")\n",
    "        self.assert_database_exists(self.schema)\n",
    "        self.assert_table_exists('bz_registered_users')\n",
    "        self.assert_table_exists('bz_gym_logins')\n",
    "        self.assert_table_exists('bz_kafka_multiplex')\n",
    "        self.assert_table_exists('sl_user_profile')\n",
    "        self.assert_table_exists('sl_workout_session')\n",
    "        self.assert_table_exists('sl_user_bins')\n",
    "        self.assert_table_exists('sl_gym_logins')\n",
    "        self.assert_table_exists('sl_heart_rate')\n",
    "        self.assert_table_exists('date_lookup')\n",
    "        self.assert_table_exists('sl_complete_workout')\n",
    "        self.assert_table_exists('sl_users')\n",
    "        self.assert_table_exists('gd_workout_bpm_summary')\n",
    "        #self.assert_table_exists('gd_gym_summary')\n",
    "        print('Done.')\n",
    "\n",
    "obj = HelperTestSuit()\n",
    "obj.run_testcases()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "helper file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}