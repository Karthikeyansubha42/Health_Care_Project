{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433267cd-a98a-48f1-be9b-a966c3e9267b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./config file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d19e4d3-b87b-4e4e-a876-b470df0621a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class SetupHelper():\n",
    "\n",
    "    def __init__(self):\n",
    "        conf = ConfigModule()\n",
    "        self.data_dir = f\"{conf.base_file_path}/data\"\n",
    "        self.checkpoint_dir = f\"{conf.base_checkpoint_path}/checkpoint\"\n",
    "        self.catalog = conf.environment\n",
    "        self.schema = conf.db_name\n",
    "        self.database_intialize = False\n",
    "\n",
    "    def cleanup(self):\n",
    "        a = input('Do you want to cleanup the database and data/checkpoint locations? (y/n): ')\n",
    "        if a.lower()=='y':\n",
    "            print('Cleaning process:', )\n",
    "            print(f'\\tDropping database {self.catalog}.{self.schema} ...', end='')\n",
    "            spark.sql(f\"DROP DATABASE IF EXISTS {self.catalog}.{self.schema} CASCADE\")\n",
    "            print('Done.')\n",
    "            '''print(f'\\tDeleting & recreate data_dir {self.data_dir} ...', end='')\n",
    "            dbutils.fs.rm(self.data_dir, True)\n",
    "            dbutils.fs.mkdirs(self.data_dir)\n",
    "            print('Done.')'''\n",
    "            print(f'\\tDeleting & recreate checkpoint_dir {self.checkpoint_dir} ...', end='')\n",
    "            dbutils.fs.rm(self.checkpoint_dir, True)\n",
    "            dbutils.fs.mkdirs(self.checkpoint_dir)\n",
    "            print('Done.')\n",
    "        else:\n",
    "            print('Skipping cleaning process!!!')\n",
    "    \n",
    "    def create_database(self):\n",
    "        spark.catalog.clearCache()\n",
    "        print(f\"\\tCreating database {self.catalog}.{self.schema} ...\", end='')\n",
    "        spark.sql(f\"create database if not exists {self.catalog}.{self.schema}\")\n",
    "        spark.sql(f\"use {self.catalog}.{self.schema}\")\n",
    "        self.database_intialize = True\n",
    "        print('Done.')\n",
    "    \n",
    "    def create_registered_user_bz(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.bz_registered_users...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.bz_registered_users (\n",
    "                        user_id bigint, \n",
    "                        device_id bigint, \n",
    "                        mac_address string, \n",
    "                        registration_timestamp string,\n",
    "                        load_time timestamp,\n",
    "                        source_file string\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "\n",
    "    def create_gym_logins_bz(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.bz_gym_logins...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.bz_gym_logins (\n",
    "                        mac_address string, \n",
    "                        gym bigint, \n",
    "                        login string, \n",
    "                        logout string,\n",
    "                        load_time timestamp,\n",
    "                        source_file string\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "\n",
    "    def create_kafka_multiplex_bz(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.bz_kafka_multiplex...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.bz_kafka_multiplex (\n",
    "                       key string, \n",
    "                       value string, \n",
    "                       topic string, \n",
    "                       partition int, \n",
    "                       offset long, \n",
    "                       timestamp long, \n",
    "                       date date, \n",
    "                       week_part string, \n",
    "                       load_time timestamp, \n",
    "                       source_file string\n",
    "                    ) PARTITIONED BY (topic, week_part)\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_users_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_users...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_users (\n",
    "                        user_id bigint, \n",
    "                        device_id bigint, \n",
    "                        mac_address string, \n",
    "                        registration_timestamp timestamp\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_gym_logins_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_gym_logins...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_gym_logins (\n",
    "                        mac_address string, \n",
    "                        gym bigint, \n",
    "                        login timestamp, \n",
    "                        logout timestamp\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_user_profile_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_user_profile...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_user_profile (\n",
    "                        user_id bigint,\n",
    "                        timestamp timestamp,\n",
    "                        dob date,\n",
    "                        sex string,\n",
    "                        gender string,\n",
    "                        first_name string,\n",
    "                        last_name string,\n",
    "                        street_address string,\n",
    "                        city string,\n",
    "                        state string,\n",
    "                        zip bigint\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_heart_rate_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_heart_rate...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_heart_rate (\n",
    "                        device_id bigint,\n",
    "                        time timestamp,\n",
    "                        heart_rate double\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_workout_session_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_workout_session...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_workout_session (\n",
    "                        user_id bigint,\n",
    "                        workout_id bigint,\n",
    "                        timestamp timestamp,\n",
    "                        action string,\n",
    "                        session_id bigint\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    \n",
    "    def create_user_bins_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_user_bins...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_user_bins (\n",
    "                        user_id bigint,\n",
    "                        device_id bigint,\n",
    "                        age string,\n",
    "                        gender string,\n",
    "                        city string,\n",
    "                        state string\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_complete_workout_sl(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.sl_complete_workout...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.sl_complete_workout (\n",
    "                        user_id bigint,\n",
    "                        workout_id bigint,\n",
    "                        session_id bigint,\n",
    "                        start_time timestamp,\n",
    "                        end_time timestamp\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "\n",
    "    def create_date_lookup(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.date_lookup...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.date_lookup (\n",
    "                        date date,\n",
    "                        week int,\n",
    "                        year int,\n",
    "                        month int,\n",
    "                        day_of_week int,\n",
    "                        day_of_month int,\n",
    "                        day_of_year int,\n",
    "                        week_part string\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_workout_bpm_summary_gd(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating table {self.catalog}.{self.schema}.gd_workout_bpm_summary...\", end='')\n",
    "            spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {self.catalog}.{self.schema}.gd_workout_bpm_summary (\n",
    "                        user_id bigint,\n",
    "                        date date,\n",
    "                        workout_id bigint,\n",
    "                        session_id bigint,\n",
    "                        age string,\n",
    "                        sex string,\n",
    "                        city string,\n",
    "                        state string,\n",
    "                        min_bpm double,\n",
    "                        avg_bpm double,\n",
    "                        max_bpm double\n",
    "                    )\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def create_gym_summary_gd(self):\n",
    "        if self.database_intialize:\n",
    "            print(f\"\\tCreating temp view {self.catalog}.{self.schema}.gd_gym_summary...\", end='')\n",
    "            spark.sql(\"\"\" create or replace view dev.hc_db.gd_gym_summary  as(\n",
    "                            with list_of_workouts as (\n",
    "                                    select user_id, collect_list(workout_id) as workout_list \n",
    "                                    from dev.hc_db.sl_complete_workout\n",
    "                                    group by user_id),\n",
    "\n",
    "                                min_in_gym as (\n",
    "                                    select b.user_id, gym, a.mac_address, login, logout, round(((unix_timestamp(logout)-unix_timestamp(login))/60),2) as minutes_in_gym, to_date(login) as date\n",
    "                                    from dev.hc_db.sl_gym_logins a\n",
    "                                    left join dev.hc_db.sl_users b on a.mac_address=b.mac_address),\n",
    "\n",
    "                                min_in_exe_per_workout_id as(\n",
    "                                    select user_id, workout_id, session_id, \n",
    "                                    round(((unix_timestamp(end_time)-unix_timestamp(start_time))/60),2) as minutes_in_exe, to_date(start_time) as date \n",
    "                                    from dev.hc_db.sl_complete_workout),\n",
    "\n",
    "                                min_in_exe as(\n",
    "                                    select user_id, date, sum(minutes_in_exe) as minutes_in_exe \n",
    "                                    from min_in_exe_per_workout_id \n",
    "                                    group by user_id, date)\n",
    "\n",
    "                            select a.gym, a.mac_address, a.date, b.workout_list, a.minutes_in_gym, c.minutes_in_exe\n",
    "                            from min_in_gym a\n",
    "                            left join list_of_workouts b on a.user_id = b.user_id\n",
    "                            left join min_in_exe c on a.user_id = c.user_id);\"\"\")\n",
    "            print('Done.')\n",
    "        else:\n",
    "            raise ReferenceError('Database not initialized')\n",
    "    \n",
    "    def launcher(self):\n",
    "        start_time = time.time()\n",
    "        self.cleanup()\n",
    "        print('Started creating tables & views: ')\n",
    "        self.create_database()\n",
    "        self.create_registered_user_bz()\n",
    "        self.create_gym_logins_bz()\n",
    "        self.create_kafka_multiplex_bz()\n",
    "        self.create_users_sl()\n",
    "        self.create_workout_session_sl()\n",
    "        self.create_user_bins_sl()\n",
    "        self.create_gym_logins_sl()\n",
    "        self.create_heart_rate_sl()\n",
    "        self.create_date_lookup()\n",
    "        self.create_user_profile_sl()\n",
    "        self.create_complete_workout_sl()\n",
    "        self.create_workout_bpm_summary_gd()\n",
    "        self.create_gym_summary_gd()\n",
    "        print(f'Done in  {int(time.time() - start_time)} seconds.')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d36f201-4421-4734-b799-bc85c4a6c6f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "class HelperTestSuit():\n",
    "\n",
    "    def __init__(self):\n",
    "        conf = ConfigModule()\n",
    "        self.data_dir = f\"{conf.base_file_path}/data\"\n",
    "        self.checkpoint_dir = f\"{conf.base_checkpoint_path}/checkpoint\"\n",
    "        self.catalog = conf.environment\n",
    "        self.schema = conf.db_name\n",
    "    \n",
    "    def assert_database_exists(self, db_name):\n",
    "        print(f'\\tChecking if database {self.schema} exits or not...', end='')\n",
    "        assert spark.sql(f\"show databases in {self.catalog}\")\\\n",
    "                           .filter(f\"databaseName='{db_name}'\").count()==1, \"Not Exist\"\n",
    "        print('Exist')\n",
    "\n",
    "    def assert_table_exists(self, table_name):\n",
    "        print(f'\\tChecking if table {table_name} exists in the {self.schema} database...', end='')\n",
    "        assert spark.sql(f\"show tables in {self.catalog}.{self.schema}\")\\\n",
    "                           .filter(f\"tableName='{table_name}' and isTemporary=False\").count()==1, \"Not Exist\"\n",
    "        print('Exist')\n",
    "    \n",
    "    def run_testcases(self):\n",
    "        print(\"Validation started...\")\n",
    "        self.assert_database_exists(self.schema)\n",
    "        self.assert_table_exists('bz_registered_users')\n",
    "        self.assert_table_exists('bz_gym_logins')\n",
    "        self.assert_table_exists('bz_kafka_multiplex')\n",
    "        self.assert_table_exists('sl_user_profile')\n",
    "        self.assert_table_exists('sl_workout_session')\n",
    "        self.assert_table_exists('sl_user_bins')\n",
    "        self.assert_table_exists('sl_gym_logins')\n",
    "        self.assert_table_exists('sl_heart_rate')\n",
    "        self.assert_table_exists('date_lookup')\n",
    "        self.assert_table_exists('sl_complete_workout')\n",
    "        self.assert_table_exists('sl_users')\n",
    "        self.assert_table_exists('gd_workout_bpm_summary')\n",
    "        #self.assert_table_exists('gd_gym_summary')\n",
    "        print('Done.')\n",
    "\n",
    "#obj = HelperTestSuit()\n",
    "#obj.run_testcases()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8562958330612008,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "helper file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}